{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day III.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8m7EraLmo1n"
      },
      "source": [
        "# Day III\n",
        "\n",
        "In today's lesson, we will go over how to train and test deep learning models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICJz0Mf4m1mr"
      },
      "source": [
        "First begin by performing the necessary installation of pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6TCmH0Img54"
      },
      "source": [
        "!pip3 install torch torchvision\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7HPamW0yXXx"
      },
      "source": [
        "## Initial Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Srhk9ghzm86V"
      },
      "source": [
        "Now, we are needing to import the pytorch library with some subfunctions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EARbyyiOnLAf"
      },
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOINXtAEnPyP"
      },
      "source": [
        "Now we are going to use a well-known dataset called MNIST. We will use the dataset function and access the dataset. We will use that function to load a training dataset and a testing dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXXbLCPhnuIe"
      },
      "source": [
        "# Create test and training sets\n",
        "train = datasets.MNIST('', train=True, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor()\n",
        "                       ]))\n",
        "\n",
        "test = datasets.MNIST('', train=False, download=True,\n",
        "                       transform=transforms.Compose([\n",
        "                           transforms.ToTensor()\n",
        "                       ]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqJz4sPzn9vW"
      },
      "source": [
        "Once extracted, we now need to place the dataset into the pytorch data pipeline. This is a fancy way of saying that the dataset needs to be configured properly to be loaded while we are training. In this case, we are slicing the data into smaller training and test sets where each set has a batch size of 10."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5DOTKfHoP7V"
      },
      "source": [
        "# This section will shuffle our input/training data so that we have a randomized shuffle of our data and do not risk feeding data with a pattern. Anorther objective here is to send the data in batches. This is a good step to practice in order to make sure the neural network does not overfit our data. NNâ€™s are too prone to overfitting just because of the exorbitant amount of data that is required. For each batch size, the neural network will run a back propagation for new updated weights to try and decrease loss each time.\n",
        "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
        "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5Z5Xps8oUJf"
      },
      "source": [
        "Now, our next goal is to actually create the neural network model. It is standard practive in Pytorch to create a neural network python class. Within the class, we initialize, using the init function within our class, the layers we need. \n",
        "\n",
        "Notice that within the init function, we are currently using only one type of layer known as linear. From what we learned previously in the slides, linear is in fact known as fully-connect layers; linear and fully-connected are synonymous.\n",
        "\n",
        "The forward function is used to determine the order of the layers that were initialized in the init function. The neural network is constructed like this: input(28x28) -> fc1 -> output(64) -> fc2 -> output(64) -> fc3 -> output(64)-> fc1 -> output(10) -> softmax layer -> final output(10)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoxRVn05p5We"
      },
      "source": [
        "# Initialize our neural net\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 64)\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.fc4(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyPBTiep9wf"
      },
      "source": [
        "Now our goal is to initialize the neural network and create our loss function and optimizer. It is typically standard for a classification problem to begin with a cross-entropy loss function and for the optimizer to be ADAM.\n",
        "\n",
        "lr is an acronym for learning rate. This is a value that we determine and can tweak accordingly. typically the learning rate by default is either 0.001 or 0.0001."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSAubj_-tYqq"
      },
      "source": [
        "\n",
        "# Calculate our neural network, loss function, and optimizer\n",
        "net = Net() \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9YJtR9Ctfe4"
      },
      "source": [
        "NOW IT IS TIME TO BEGIN TRAINING!\n",
        "\n",
        "The First for loop is how many epochs or cycles we perform to train the data. The second loop iterates through the batch size.\n",
        "\n",
        "\n",
        "\n",
        "1.   Collect the input and output label values from the data\n",
        "2.   set gradients to 0 before calculating the loss; this is a necessary step in order to properly calculate the loss. \n",
        "3. pass the input through the neural network\n",
        "4. calculate the loss\n",
        "5. send the error or loss through the network to update the neural network\n",
        "6. update the optimizer\n",
        "7. repeat\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye5WxQFemjLy"
      },
      "source": [
        "\n",
        "\n",
        "# begin training.\n",
        "# with torch.no_grad(): # sets gradients to 0 before calculating loss.\n",
        "for epoch in range(5): # we use 5 epochs\n",
        "    for data in trainset:  # `data` is a batch of data\n",
        "        X, y = data  # X is the batch of features, y is the batch of targets.\n",
        "\n",
        "        net.zero_grad()  \n",
        "\n",
        "        output = net(X.view(-1,784))  # pass in the reshaped batch (recall they are 28x28 atm, -1 is needed to show that output can be n-dimensions. This is PyTorch exclusive syntax)\n",
        "\n",
        "        loss = criterion(output, y)  # calc and grab the loss value\n",
        "\n",
        "        loss.backward()  # apply this loss backwards thru the network's parameters\n",
        "        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n",
        "    print(loss)  \n",
        "\n",
        "### Output:\n",
        "### tensor(0.6039, grad_fn=)\n",
        "### tensor(0.1082, grad_fn=)\n",
        "### tensor(0.0194, grad_fn=)\n",
        "### tensor(0.4282, grad_fn=)\n",
        "### tensor(0.0063, grad_fn=)\n",
        "\n",
        "\n",
        "### Output: \n",
        "### Accuracy:  0.915"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT3gBeV6wsc_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btk0w9LPx7BC"
      },
      "source": [
        "Calculate the Accuracy. Now to calculate the accuracy, the process is still the same, however, there is no need to 4-6. Rather, the process is to send the input into the neural network and then calculate the accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfKNLbv7x9L1"
      },
      "source": [
        "# Get the Accuracy\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testset:\n",
        "        X, y = data\n",
        "        output = net(X.view(-1,784))\n",
        "        #print(output)\n",
        "        for idx, i in enumerate(output):\n",
        "            #print(torch.argmax(i), y[idx])\n",
        "            if torch.argmax(i) == y[idx]:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "\n",
        "print(\"Accuracy: \", round(correct/total, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHt76-veycOG"
      },
      "source": [
        "## Convolutional Network Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UkeoiNvykBo"
      },
      "source": [
        "First, we need to load the necessary pytorch libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9efXXJlygoU"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saIPhe-gyyk5"
      },
      "source": [
        "Now we will begin loading the dataset and necessary variables for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR_BT_r4ypRd"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLaGu7rgy1c5"
      },
      "source": [
        "Let's review  some of the training images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCta5_qRy6r_"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "# print labels\n",
        "print(' '.join('%5s' % classes[labels[j]] for j in range(batch_size)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZeaQBfEy8_0"
      },
      "source": [
        "Now we will need to define a Convolutional Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEbg1VYgzC3b"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Q4wXha_0hHQ"
      },
      "source": [
        "Now it is time to send the network to the gpu to train faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJJTPqxp0gif"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "\n",
        "print(device)\n",
        "\n",
        "net.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7PQjpZVzEof"
      },
      "source": [
        "Now we will need to define the loss function criterion and the optimizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qhgav_Y2zJUw"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2c_23G6zLKU"
      },
      "source": [
        "Let's begin to train the network. This is when things start to get interesting. We simply have to loop over our data iterator, and feed the inputs to the network and optimize.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-3IvJ5KzN12"
      },
      "source": [
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        inputs, labels = data[0].to(device), data[1].to(device) # send to GPU\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlTKcKBjzQVp"
      },
      "source": [
        "Before we continue, it would be great to save our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1GvovHvzS9K"
      },
      "source": [
        "PATH = './cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMsdhHsUzUpi"
      },
      "source": [
        "Let's test the network on our test data. We have trained the network for 2 passes over the training dataset. But we need to check if the network has learnt anything at all.\n",
        "\n",
        "We will check this by predicting the class label that the neural network outputs, and checking it against the ground-truth. If the prediction is correct, we add the sample to the list of correct predictions.\n",
        "\n",
        "Okay, first step. Let us display an image from the test set to get familiar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFHEWM4jzXuB"
      },
      "source": [
        "dataiter = iter(testloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# print images\n",
        "imshow(torchvision.utils.make_grid(images))\n",
        "print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIfNJWrgzjF9"
      },
      "source": [
        "Next, letâ€™s load back in our saved model (note: saving and re-loading the model wasnâ€™t necessary here, we only did it to illustrate how to do so):\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FstN-5f6zkma"
      },
      "source": [
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVmp4-JYzmG2"
      },
      "source": [
        "Okay, now let us see what the neural network thinks these examples above are:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35qv3cq-zndT"
      },
      "source": [
        "outputs = net(images)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN2b43Kizqan"
      },
      "source": [
        "The outputs are energies for the 10 classes. The higher the energy for a class, the more the network thinks that the image is of the particular class. So, letâ€™s get the index of the highest energy:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boU0A0W4zsC4"
      },
      "source": [
        "_, predicted = torch.max(outputs, 1)\n",
        "\n",
        "print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n",
        "                              for j in range(4)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjT_Qw6aztgL"
      },
      "source": [
        "The results seem pretty good.\n",
        "\n",
        "Let us look at how the network performs on the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRVXPOMZzvrd"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = net(images)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8EhBbJVzxZ1"
      },
      "source": [
        "That looks way better than chance, which is 10% accuracy (randomly picking a class out of 10 classes). Seems like the network learnt something.\n",
        "\n",
        "Hmmm, what are the classes that performed well, and the classes that did not perform well:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t9NcY8lzzXs"
      },
      "source": [
        "# prepare to count predictions for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "# again no gradients needed\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        # collect the correct predictions for each class\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "\n",
        "# print accuracy for each class\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(\"Accuracy for class {:5s} is: {:.1f} %\".format(classname,\n",
        "                                                   accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}